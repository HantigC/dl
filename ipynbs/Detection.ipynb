{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133ebaa9-d854-493e-8f6e-474197f7e8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/environment/Projects/dl\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab479a4-166f-4d1b-b6bf-966016542236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./deps/light-torch\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: light-torch\n",
      "  Building wheel for light-torch (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for light-torch: filename=light_torch-0.0.1-py3-none-any.whl size=6012 sha256=0720e01589dbba642c2c88e82b39d2680668a5df03264a3b9e5a014c0f4e855f\n",
      "  Stored in directory: /home/stefan-cristianhantig/.cache/pip/wheels/76/4b/a5/ce53c36ade717c024783dce5309d843ab5a592d06a292dafc2\n",
      "Successfully built light-torch\n",
      "Installing collected packages: light-torch\n",
      "  Attempting uninstall: light-torch\n",
      "    Found existing installation: light-torch 0.0.1\n",
      "    Uninstalling light-torch-0.0.1:\n",
      "      Successfully uninstalled light-torch-0.0.1\n",
      "Successfully installed light-torch-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install deps/light-torch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5cf715-bc07-4c82-8750-21e6808a9763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from light_torch.module import Module\n",
    "from src.object_detection.yolov1 import YoloV1Backbone, YoloV1, ObjectDetectionModule\n",
    "from src.classification.models.resnet import Resnet\n",
    "from torchsummary import summary\n",
    "import json\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from functools import lru_cache\n",
    "from torchvision.transforms import Compose\n",
    "from IPython.core.debugger import set_trace\n",
    "from src.object_detection.data.coco import CocoDetectionDataset, make_id2category_map, collect_annotations\n",
    "from src.object_detection.utils.visualization import draw_bbox_xyhw, draw_rect_xywh_, draw_grid_, draw_boxes_tlbr_\n",
    "import torch\n",
    "from src.transform import to_rgb, to_torch, resize\n",
    "from functools import partial\n",
    "from torch.nn import functional as F\n",
    "from collections import defaultdict\n",
    "from src.object_detection.yolov1 import YoloV1Loss\n",
    "from collections import defaultdict\n",
    "from light_torch.train import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from src.object_detection.data.utils import pad_collate\n",
    "from src.torchx import target_to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f687527d-f21f-4ba1-b5cc-96b9c6d9f9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_ANNOTATION_FILE = \"resources/object_detection/annotations/instances_train2014.json\"\n",
    "VAL_ANNOTATION_FILE = \"resources/object_detection/annotations/instances_val2014.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce58ee84-0bc9-424b-a8b6-8493454dad45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(TRAIN_ANNOTATION_FILE, \"r\") as json_file:\n",
    "    train_coco = json.load(json_file)\n",
    "    \n",
    "with open(VAL_ANNOTATION_FILE, \"r\") as json_file:\n",
    "    val_coco = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b02f16e-6c96-4274-a8b1-bd5fea642111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = Path(\"./resources/object_detection/train2014/\")\n",
    "VAL_IMAGES_PATH = Path(\"./resources/object_detection/val2014/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086c6a96-adec-4b67-b173-5f144d554d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_transform = Compose([\n",
    "    to_torch,\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94c629f-1b88-42b5-8fb8-533d043f23e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_categories(annotation_dict, categories=None):\n",
    "    return annotation_dict[\"category_id\"] in categories\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cbec879-d6f6-474e-aaec-656832262fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "coco_train_ds = CocoDetectionDataset(\n",
    "    train_coco, \n",
    "    TRAIN_IMAGES_PATH,\n",
    "    transform=to_torch,\n",
    "    target_transform=target_to_torch,\n",
    "    select_annotation=partial(select_categories, categories=[78, 79]),\n",
    "    \n",
    ")\n",
    "\n",
    "val_train_ds = CocoDetectionDataset(\n",
    "    val_coco, \n",
    "    VAL_IMAGES_PATH,\n",
    "    transform=to_torch,\n",
    "    target_transform=target_to_torch,\n",
    "    select_annotation=partial(select_categories, categories=[78, 79]),\n",
    "    \n",
    ")\n",
    "train_dl = DataLoader(coco_train_ds, collate_fn=pad_collate, batch_size=2, shuffle=True)\n",
    "val_dl = DataLoader(val_train_ds, collate_fn=pad_collate, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9844f5ec-12b2-48e9-9083-1755a5c0650b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#resnet = Resnet()\n",
    "yolo = YoloV1(coco_train_ds.categories_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e0afbb-b78b-4cce-8b01-ca43e16ae48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolov1_loss = YoloV1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee31d8c-da82-4839-be94-4831c3b7d13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec604fc5-39da-402d-bd00-f5276e965c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff7d8201-142a-4aed-8464-39bcc512290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_detection = ObjectDetectionModule(\n",
    "    yolo, \n",
    "    yolov1_loss, \n",
    "    coco_train_ds.id2name_map, \n",
    "    iou_threshold=0.2,\n",
    "    transform=Compose([TF.Normalize((0, 0, 0), (1, 1, 1))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c5d8e6f-afe4-4cb4-a224-dcf2d6e3adbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(obj_detection.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bc56023-6e6f-4210-a2af-8fa100e54040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    obj_detection, \n",
    "    epochs=10,\n",
    "    accumulation=4,\n",
    "    optimizer=optimizer,\n",
    "    name=\"yolov1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26553a5e-ecad-4112-bbd9-1ee2154e20ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trainer(\n",
       "  (module): ObjectDetectionModule(\n",
       "    (model): YoloV1(\n",
       "      (backbone): YoloV1Backbone(\n",
       "        (st_conv): Conv2dSamePadding(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "        (st_maxpol): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (net): Sequential(\n",
       "          (conv2): Conv2dSamePadding(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation2): LeakyReLU(negative_slope=0.01)\n",
       "          (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (identity_conv31): Conv2dSamePadding(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv31): Conv2dSamePadding(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation31): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv32): Conv2dSamePadding(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv32): Conv2dSamePadding(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation32): LeakyReLU(negative_slope=0.01)\n",
       "          (pool3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (identity_conv41): Conv2dSamePadding(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv41): Conv2dSamePadding(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation41): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv42): Conv2dSamePadding(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv42): Conv2dSamePadding(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation42): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv43): Conv2dSamePadding(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv43): Conv2dSamePadding(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation43): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv44): Conv2dSamePadding(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv44): Conv2dSamePadding(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation44): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv45): Conv2dSamePadding(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv45): Conv2dSamePadding(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation45): LeakyReLU(negative_slope=0.01)\n",
       "          (pool4): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (identity_conv51): Conv2dSamePadding(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv51): Conv2dSamePadding(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation51): LeakyReLU(negative_slope=0.01)\n",
       "          (identity_conv52): Conv2dSamePadding(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (conv52): Conv2dSamePadding(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation52): LeakyReLU(negative_slope=0.01)\n",
       "          (conv53): Conv2dSamePadding(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation53): LeakyReLU(negative_slope=0.01)\n",
       "          (pool5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (conv61): Conv2dSamePadding(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation61): LeakyReLU(negative_slope=0.01)\n",
       "          (conv62): Conv2dSamePadding(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (activation62): LeakyReLU(negative_slope=0.01)\n",
       "        )\n",
       "      )\n",
       "      (to_grid): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "      (conf): LazyConv2d(\n",
       "        (net): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (loss): YoloV1Loss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24338fc-d65a-45d9-ad6e-73823487291b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67600618dd09468295d444dea6fed477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch  001:   0%|          | 0/1227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE TRAIN: Epoch:  001\n",
      "{'box_loss': 24.84475357599478, 'classification_loss': 7.072964195663014, 'confidence_loss': 0.0016038249361499967, 'loss': 31.919321596593843}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fbe5b5c5d4e3da556282b3b32644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val epoch  001:   0%|          | 0/603 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE VAL: Epoch:  001\n",
      "{'box_loss': 25.032044027781435, 'classification_loss': 6.818803132766515, 'confidence_loss': 0.0004256292899296278, 'loss': 31.851272789837864, 'mAP_0.5': 3.80087255292722e-05, 'mAP_0.55': 2.8323926818561035e-05, 'mAP_0.6': 2.908595302112031e-05, 'mAP_0.65': 5.9153984264796995e-06, 'mAP_0.7': 5.9340665861611635e-06, 'mAP_0.75': 1.948246772729221e-06, 'mAP_0.8': 9.741233863646105e-07, 'mAP_0.85': 9.741233863646105e-07, 'mAP_0.9': 5.902933344666966e-07, 'mAP_0.95': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c5382a3ea449fb96a4069b26572c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch  002:   0%|          | 0/1227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(train_dl, val_data=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b866623-8f3e-4034-8c20-8c07ad75eb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obj_detection.val_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8982a732-611d-46de-bff2-4f04de61bb48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_detection.on_val_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48c7608e-6b73-489a-bdcd-ce5d044391a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'box_loss': 49.54795295827014,\n",
       " 'classification_loss': 11.735513687133789,\n",
       " 'confidence_loss': 12.811930656433105,\n",
       " 'loss': 74.09539730183704,\n",
       " 'mAP_0.5': 0.0,\n",
       " 'mAP_0.55': 0.0,\n",
       " 'mAP_0.6': 0.0,\n",
       " 'mAP_0.65': 0.0,\n",
       " 'mAP_0.7': 0.0,\n",
       " 'mAP_0.75': 0.0,\n",
       " 'mAP_0.8': 0.0,\n",
       " 'mAP_0.85': 0.0,\n",
       " 'mAP_0.9': 0.0,\n",
       " 'mAP_0.95': 0.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detection.get_batch_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06a10bca-7006-4936-9fd7-0f715b308fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'box_loss': 49.54795295827014,\n",
       " 'classification_loss': 11.735513687133789,\n",
       " 'confidence_loss': 12.811930656433105,\n",
       " 'loss': 74.09539730183704,\n",
       " 'mAP_0.5': 0.0,\n",
       " 'mAP_0.55': 0.0,\n",
       " 'mAP_0.6': 0.0,\n",
       " 'mAP_0.65': 0.0,\n",
       " 'mAP_0.7': 0.0,\n",
       " 'mAP_0.75': 0.0,\n",
       " 'mAP_0.8': 0.0,\n",
       " 'mAP_0.85': 0.0,\n",
       " 'mAP_0.9': 0.0,\n",
       " 'mAP_0.95': 0.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_detection.get_epoch_log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
